<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>blog.wamser.eu - Growing 'Lists' in R</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">blog.wamser.eu</a>
            </div>
            <div id="navigation">
                <a href="../about.html">About</a>
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <h1>Growing 'Lists' in R</h1>

            <div class="info">
    Posted on February  1, 2016
    
</div>

<p><strong>TL;DR</strong> Growing things like vectors or lists on-demand is a performance killer in R. You should always pre-allocate. If you really cannot determine the size of your objects beforehand, linking lists may be your rescue.</p>
<p>So, we all reached the point where we worked on our data chunk by chunk and the result of every chunk went into a list or vector. And, of course we all read <a href="http://www.burns-stat.com/pages/Tutor/R_inferno.pdf">R Inferno</a>, chapters 2 and 3, so we know that we should vectorise and pre-allocate a result vector or list of appropriate size when looping. (Vectorising usually takes care of the pre-allocation.)</p>
<p>But there are some cases, where pre-allocation is just not possible. Just assume that you have a large corpus of log files and you want to count how many times users accessed your service in each hour of the day during the last eight years. That would be about 70128 datapoints per user. Multiply that with 20 Million users and you get a huge <code>data.frame</code> (a few terabytes). So, in theory you could allocate a vector of size 70128 for each of your user, but most entries would be zero and you would waste a lot of memory.</p>
<p>On the other hand, you could just produce a list with an entry for each user and grow a named vector (named by day/hour) at this list position dynamically.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create a list with users as keys</span>
<span class="co"># set each list element &lt;- vector(&quot;numeric&quot;,0L)</span>

<span class="co"># in the loop parsing the logfiles</span>
<span class="co"># ... </span>
  counts[[<span class="st">&quot;users&quot;</span>]][[timestamp]]&lt;-interactionsInThisHour
<span class="co"># ...</span></code></pre></div>
<p>And this will fast-track you to performance-hell. Why? Because every time a new entry is added to the named vector, R creates a new instance of the vector and copies over all old data. (Same with lists.)<br />
As soon as you have some power-users, your have a nice stress-test for your memory, but it will take ages to churn through your log files.</p>
<p>You will get a glimpse of the horrors that lurk in this style of programming, when we get to performance numbers in a second. (Or have a look at the above mentioned <a href="http://www.burns-stat.com/pages/Tutor/R_inferno.pdf">book of horrors</a>)</p>
<p>But hey, there’s no reason for a blog post, when there isn’t a solution at hand (and I’ve got one in pure R for you).</p>
<p>Here’s a simplified version of our problem:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> extby1 &lt;-<span class="st"> </span>function(s) {
   ret &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>)
   for (i in s) ret[i] &lt;-<span class="st"> </span><span class="dv">1</span>
   <span class="kw">return</span>(ret)
 }</code></pre></div>
<p>A first step towards a solution might be increasing the return vector in chunks. The bigger the chunks, the less copying. All for the price of keeping track of the free capacity. This helps indeed, but not much. (We might get some additional unused vector entries, but we do not care here.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">extbyN &lt;-<span class="st"> </span>function(s,n) {
   ret &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>,n)
   rlen &lt;-<span class="st"> </span>n
   rpos &lt;-<span class="st"> </span><span class="dv">1</span>
   for (i in s) {
     ret[rpos] &lt;-<span class="st"> </span><span class="dv">1</span>
     <span class="kw">names</span>(ret)[rpos]&lt;-i
     rpos &lt;-<span class="st"> </span>rpos<span class="dv">+1</span>
     if (rpos ==<span class="st"> </span>rlen) {
       ret &lt;-<span class="st"> </span><span class="kw">c</span>(ret,<span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>,n))
       rlen &lt;-<span class="st"> </span>rlen +<span class="st"> </span>n
     }
   }
   <span class="kw">return</span>(ret)
 }</code></pre></div>
<p>Finally we could simulate a one-way linked list by nesting lists, then <code>unlist</code>ing those lists.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> extList &lt;-<span class="st"> </span>function(s) {
   li.v &lt;-<span class="kw">vector</span>(<span class="st">&quot;list&quot;</span>)
   li.n &lt;-<span class="kw">vector</span>(<span class="st">&quot;list&quot;</span>)
   for (i in s) {
     li.v$val &lt;-<span class="st"> </span><span class="dv">1</span>
     li.n$val &lt;-<span class="st"> </span>i
     li.v &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">prev=</span>li.v)
     li.n &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">prev=</span>li.n)
   }
   ret&lt;-<span class="kw">unlist</span>(li.v,<span class="dt">use.names =</span> F)
   <span class="kw">names</span>(ret)&lt;-<span class="kw">unlist</span>(li.n,<span class="dt">use.names =</span> F)
   <span class="kw">return</span>(ret)
 }</code></pre></div>
<p>Guess which one is fastest?</p>
<p>Let’s look at the performance for a single user. (Remember, you have 20 million of them!)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(microbenchmark)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">as.character</span>(<span class="dv">1</span>:<span class="dv">10000</span>)
 
<span class="kw">microbenchmark</span>(
   <span class="dt">single=</span><span class="kw">extby1</span>(s),
   <span class="dt">chunk=</span><span class="kw">extbyN</span>(s,<span class="dv">1000</span>),
   <span class="dt">lists=</span><span class="kw">extList</span>(s),
   <span class="dt">times=</span><span class="dv">10</span>
)</code></pre></div>
<pre><code>## Unit: milliseconds
##    expr        min         lq      mean     median         uq        max neval
##  single 3058.29194 3086.62544 3111.7070 3115.64737 3129.85283 3158.73425    10
##   chunk  419.25559  431.06599  450.3283  442.83384  480.92187  485.12857    10
##   lists   47.21081   48.44823   51.1332   50.49854   53.17632   59.05372    10</code></pre>
<p>So we are talking about orders of magnitude. With the single-step extension clearly out of question here, the chunk option also loses to the list variant in performance and also in code clarity, I think.</p>
<p>Talking of code clarity: The linked-lists variant behaves a bit like lists in Haskell. Every time we have a new element, we just tack it on to the list at the same end. We can also easily produce a version that returns reversed lists:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> extListRev &lt;-<span class="st"> </span>function(s) {
   li.v &lt;-<span class="kw">vector</span>(<span class="st">&quot;list&quot;</span>)
   li.n &lt;-<span class="kw">vector</span>(<span class="st">&quot;list&quot;</span>)
   for (i in s) {
     li.v$val &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(i)
     li.n$val &lt;-<span class="st"> </span>i
     li.v &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">val=</span><span class="ot">NULL</span>,<span class="dt">prev=</span>li.v)
     li.n &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">val=</span><span class="ot">NULL</span>,<span class="dt">prev=</span>li.n)
   }
   ret&lt;-<span class="kw">unlist</span>(li.v,<span class="dt">use.names =</span> F)
   <span class="kw">names</span>(ret)&lt;-<span class="kw">unlist</span>(li.n,<span class="dt">use.names =</span> F)
   <span class="kw">return</span>(ret)
 }

 <span class="kw">extListRev</span>(<span class="dv">1</span>:<span class="dv">10</span>)</code></pre></div>
<pre><code>## 10  9  8  7  6  5  4  3  2  1 
## 10  9  8  7  6  5  4  3  2  1</code></pre>
<p>See, what I did here? Before, each list element in the chain had first a pointer to the previous element, then the value was added. By explicitly creating the value field first I swapped the indices of those entries. Now, when <code>unlist</code> recurses, elements will be returned in reverse order. Nice, eh?</p>
<p>But this post is about performance, so let’s get back to that topic. For quite some time now (since 2.15 iirc) R sports a byte code compiler. Let’s check whether we can get some more speedup.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(compiler)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">extby1c &lt;-<span class="st"> </span><span class="kw">cmpfun</span>(extby1)
extbyNc &lt;-<span class="st"> </span><span class="kw">cmpfun</span>(extbyN)
extListc &lt;-<span class="st"> </span><span class="kw">cmpfun</span>(extList)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">as.character</span>(<span class="dv">1</span>:<span class="dv">10000</span>)
<span class="kw">microbenchmark</span>(
  <span class="dt">uncom=</span><span class="kw">extby1</span>(s),
  <span class="dt">com=</span><span class="kw">extby1c</span>(s),
  <span class="dt">times=</span><span class="dv">10</span>
)</code></pre></div>
<pre><code>## Unit: seconds
##   expr      min       lq     mean   median       uq      max neval
##  uncom 2.785569 2.795639 2.922520 2.813646 2.924904 3.469368    10
##    com 2.727951 2.765242 2.836841 2.773233 2.832985 3.264978    10</code></pre>
<p>Growing a vector step-by-step is (obviously) still not a good idea.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">as.character</span>(<span class="dv">1</span>:<span class="dv">10000</span>)
<span class="kw">microbenchmark</span>(
   <span class="dt">single=</span><span class="kw">extby1c</span>(s),
   <span class="dt">chunk100=</span><span class="kw">extbyNc</span>(s,<span class="dv">100</span>),
   <span class="dt">chunk1000=</span><span class="kw">extbyNc</span>(s,<span class="dv">1000</span>),
   <span class="dt">chunk10000=</span><span class="kw">extbyNc</span>(s,<span class="dv">10000</span>),
   <span class="dt">lists=</span><span class="kw">extListc</span>(s),
   <span class="dt">times=</span><span class="dv">10</span>
)</code></pre></div>
<pre><code>## Unit: milliseconds
##        expr        min         lq       mean     median        uq        max neval
##      single 2749.28302 2793.42233 2865.03523 2830.56872 2868.1223 3237.38729    10
##    chunk100  324.36390  333.56473  369.62350  382.69027  392.4730  397.01763    10
##   chunk1000  288.74192  305.99301  351.10467  353.63883  372.0679  418.11485    10
##  chunk10000  557.07741  571.72878  601.54277  585.69835  601.5508  702.84554    10
##       lists   17.82125   18.23444   20.91505   19.13438   21.5660   31.68218    10</code></pre>
<p>Suprisingly not much dependence on the chunk size, aside from the biggest version. And lists still rock.</p>
<p>What about bigger inputs? Before we run the benchmarks, let’s define a function that pre-allocates, so we have a baseline performance. (Actually, that was already hidden above, when chunk size was the same as input size.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> baseline &lt;-<span class="st"> </span>function(s) {
   ret &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>,<span class="kw">length</span>(s))
   <span class="kw">names</span>(ret)&lt;-s
   for (i in s) {
     ret[[i]] &lt;-<span class="st"> </span><span class="dv">1</span>
   }
   <span class="kw">return</span>(ret)
 }

baselinec &lt;-<span class="st"> </span><span class="kw">cmpfun</span>(baseline)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">as.character</span>(<span class="dv">1</span>:<span class="dv">100000</span>)

<span class="kw">microbenchmark</span>(
  <span class="dt">chunk=</span><span class="kw">extbyN</span>(s,<span class="dv">1000</span>),
  <span class="dt">lists=</span><span class="kw">extList</span>(s),
  <span class="dt">chunk.comp=</span><span class="kw">extbyNc</span>(s,<span class="dv">1000</span>),
  <span class="dt">lists.comp=</span><span class="kw">extListc</span>(s),
  <span class="dt">base=</span><span class="kw">baseline</span>(s),
  <span class="dt">basec=</span><span class="kw">baselinec</span>(s),
  <span class="dt">times=</span><span class="dv">10</span>
)</code></pre></div>
<pre><code>## Unit: milliseconds
##        expr         min          lq        mean      median          uq         max neval
##       chunk  48001.4479  48856.5420  49263.6128  48961.3576  49265.0617  51771.8221    10
##       lists    558.2507    722.8191    826.5523    816.8246    886.7426   1172.8300    10
##  chunk.comp  47335.0755  48274.2159  52287.6820  48636.1823  49118.8342  86030.3053    10
##  lists.comp    223.3270    250.9604    369.8465    311.4613    432.3449    787.4957    10
##        base 123226.0538 123577.8123 124717.3147 124134.3694 125057.5396 129645.6730    10
##       basec 120432.9313 123388.9605 124631.0460 124274.2862 126515.6508 129148.5368    10</code></pre>
<p><a href="https://xkcd.com/1627/">WOOSH.</a></p>
<p>So just in case you find yourself in the situation where you need to grow a list of objects dynamically: Linking lists might be just good enough for you. And the R Compiler is your friend.</p>
<p>(Still makes me wonder, why the pre-allocation version is slower than growing in chunks. But then, I’m not a R whiz and there is probably some misconception on how this works from my side.)</p>
<h2 id="a-note-on-memory-usage">A Note on Memory Usage</h2>
<p>While the runtime performance of the above linked lists scales pretty linear with the number of elements you stuff into them, there is a drawback when it comes to memory usage: In the worst case (e.g. when you are just collecting integers and thus have the most overhead for creating those lists), you need about 12 times the amount of memory of what the corresponding collapsed list/vector needs. So before you run out of memory you can, (repeatedly) after collecting a lot of values, collapse (<code>unlist</code>) all the values into a flat list/vector and assign that to the <code>$val</code> slot, then go on with a fresh start for the list chain. <code>unlist</code> will play along nicely and merge single values and vectors of values as expected.</p>
<h2 id="bootnote">Bootnote</h2>
<p>There’s also always the option to open a temporary file on a ramdisk and appending to that. But honestly, then you are at a point where you might want to externalise the process and only load the results into R. Plus, with 20 million open files you might hit some limit on file handles.</p>

        </div>
        <div id="footer">
            <a href="../impressum.html">Impressum</a>
        </div>
    </body>
</html>
